{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANLY-580 Assignment 2\n",
    "\n",
    "This assignment covers concepts from ANLY-580 Modules 2 and 3 dealing with distributional semantics and language modeling. To submit this assignment, enter your answers directly in this document. For question 4 please upload a python file / notebook containing your solution.\n",
    "\n",
    "#\n",
    "**Due Dates**:\n",
    "- Section 01: Oct 12 11:59pm EST\n",
    "- Section 02: Oct 13 11:59pm EST\n",
    "- Section 03: Oct 24 11:59pm EST\n",
    "\n",
    "**Grade**: 10% (100 pts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. (10 pts) Describe why the BOW feature representation limits our ability to model human language. What aspect of language, and specifically word meaning, does BOW ignore? Do approaches like TF-IDF and PMI resolve this shortcoming in BOW?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1)** We are ignoring the importance of each word.  \n",
    "**(2)** We are losing the information of grammar(Order of words).\n",
    "\n",
    "TF-IDF solves (1) by multiplying by a factor of IDF which describes the importance of that word.  \n",
    "PMI computes the log-co-occurence and we can make use of that later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. (10 pts) The word2vec language modeling approach was perhaps the first successful method to learn meaningful word representations. How does word2vec assign/measure similarity between two words?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2vec uses different architectures to compute word embeddings for words and then compute similarity between (lists of) words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. (10 pts) Why are the inner product and cosine similarity used to measure similarity and not euclidean distance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Euclidean distance describes the dissimilarity between words.  \n",
    "And inner product and cosine similarity are good metrics to describe similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. (50 pts) Write a Python program to compute the trigram ($n=3$) probabilities of the following Dr. Suess corpus:\n",
    "\n",
    "    *Hint: See Jurafsky & Martin Chp. 3 for bigram estimation from a similar corpus*\n",
    "\n",
    "    ```\n",
    "    <s> I am Sam </s>\n",
    "    <s> Sam I am </s>\n",
    "    <s> I am Sam </s>\n",
    "    <s> I do not like green eggs and Ham </s>\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.66666667 0.         0.         0.         0.33333333 0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.66666667 0.33333333 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         1.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         1.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [1.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         1.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  1.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         1.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         1.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         1.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         1.        ]\n",
      " [0.         0.         1.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "corpus = \"<s> I am Sam </s>\\n<s> Sam I am </s>\\n<s> I am Sam </s>\\n<s> I do not like green eggs and Ham </s>\"\n",
    "sentences = corpus.split('\\n')\n",
    "word_pair_vocab = {}\n",
    "word_vocab = {}\n",
    "for sent in sentences:\n",
    "    temp_word_list = sent.split(' ')\n",
    "    for i in range(len(temp_word_list)-2):\n",
    "        temp_word_pair = temp_word_list[i] + ' ' + temp_word_list[i+1]\n",
    "        temp_word = temp_word_list[i+2]\n",
    "        if temp_word_pair not in word_pair_vocab:\n",
    "            word_pair_vocab[temp_word_pair] = len(word_pair_vocab)\n",
    "        if temp_word not in word_vocab:\n",
    "            word_vocab[temp_word]=len(word_vocab)\n",
    "\n",
    "X_stat = np.zeros((len(word_pair_vocab), len(word_vocab)))\n",
    "\n",
    "for sent in sentences:\n",
    "    temp_word_list = sent.split(' ')\n",
    "    for i in range(len(temp_word_list)-2):\n",
    "        temp_word_pair = temp_word_list[i] + ' ' + temp_word_list[i+1]\n",
    "        temp_word = temp_word_list[i+2]\n",
    "        X_stat[word_pair_vocab[temp_word_pair], word_vocab[temp_word]] += 1\n",
    "\n",
    "for i in range(X_stat.shape[0]):\n",
    "    X_stat[i, :] = X_stat[i, :] / X_stat[i, :].sum()\n",
    "print(X_stat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we have the probability matrix of trigram(without smooth parameter)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. (20 pts) Recall from Lecture 03 that the principle of maximum likelihood makes two qualifying assumptions for any dataset/model combination:\n",
    "\n",
    "    - all examples are drawn from the same distribution\n",
    "\n",
    "    - all examples are independently drawn\n",
    "\n",
    "    Which of these qualifying assumptions do we break when learning the parameters of a language model using MLE?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first one.  \n",
    "The first assertion is not essentially correctly. Given different previous words, we have a different distribution of the following words.  \n",
    "The second one is not essentially correctly, either. But we always assume (conditional) independence for words in a sentence for easier computation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
